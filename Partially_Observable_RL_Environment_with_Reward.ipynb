{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Xvfb (X virtual framebuffer) itself is a display server that performs all graphical operations in virtual memory, emulating a screen.\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install moviepy\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import time\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x-nEFRa9UQrP",
        "outputId": "55223016-9807-4c6c-be25-150e4e2d772f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+5build2).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.12/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.12/dist-packages (from pyvirtualdisplay) (1.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwDBzIquTaXp",
        "outputId": "5176657f-0032-4824-e62f-34cf52f16073",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+5build2).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.12/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.12/dist-packages (from pyvirtualdisplay==0.2.*) (1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium numpy\n",
        "# Optional: Install dependencies for video recording in Colab\n",
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install pyvirtualdisplay==0.2.*\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "from pyvirtualdisplay import Display\n",
        "import os\n",
        "\n",
        "# Start virtual display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PartiallyObservableGridEnv(gym.Env):\n",
        "    def __init__(self, maze_size=(10, 10), observation_window_size=3):\n",
        "        super().__init__()\n",
        "        self.maze_size = maze_size\n",
        "        self.window_size = observation_window_size\n",
        "        self.agent_pos = None\n",
        "        self.goal_pos = (maze_size[0] - 1, maze_size[1] - 1)\n",
        "\n",
        "        # Define Action space: 0: up, 1: down, 2: left, 3: right\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Define the observation space as a local window\n",
        "        # The observation will be a 2D array representing the local area\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1,\n",
        "                                                shape=(self.window_size, self.window_size),\n",
        "                                                dtype=int)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        # Randomize start position\n",
        "        self.agent_pos = (random.randint(0, self.maze_size[0] - 1),\n",
        "                          random.randint(0, self.maze_size[1] - 1))\n",
        "        # Ensure agent doesn't start at the goal\n",
        "        while self.agent_pos == self.goal_pos:\n",
        "            self.agent_pos = (random.randint(0, self.maze_size[0] - 1),\n",
        "                              random.randint(0, self.maze_size[1] - 1))\n",
        "\n",
        "        observation = self._get_observation()\n",
        "        info = {}\n",
        "        return observation, info\n",
        "\n",
        "    def step(self, action):\n",
        "        # Define movements\n",
        "        x, y = self.agent_pos\n",
        "        if action == 0: # Up\n",
        "            x = max(0, x - 1)\n",
        "        elif action == 1: # Down\n",
        "            x = min(self.maze_size[0] - 1, x + 1)\n",
        "        elif action == 2: # Left\n",
        "            y = max(0, y - 1)\n",
        "        elif action == 3: # Right\n",
        "            y = min(self.maze_size[1] - 1, y + 1)\n",
        "\n",
        "        self.agent_pos = (x, y)\n",
        "\n",
        "        # Check if goal is reached\n",
        "        terminated = (self.agent_pos == self.goal_pos)\n",
        "        reward = 1.0 if terminated else 0.0\n",
        "\n",
        "        observation = self._get_observation()\n",
        "        truncated = False # No episode length limit in this example\n",
        "        info = {}\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def _get_observation(self):\n",
        "        # Extract the local window around the agent\n",
        "        obs = np.zeros((self.window_size, self.window_size), dtype=int)\n",
        "        half_window = self.window_size // 2\n",
        "\n",
        "        for i in range(self.window_size):\n",
        "            for j in range(self.window_size):\n",
        "                env_x = self.agent_pos[0] + i - half_window\n",
        "                env_y = self.agent_pos[1] + j - half_window\n",
        "\n",
        "                # Check bounds and mark goal in observation if visible\n",
        "                if (env_x, env_y) == self.goal_pos:\n",
        "                    obs[i, j] = 1 # 1 indicates the goal is in this spot\n",
        "                # Edges can be implicitly handled as 0 (empty space)\n",
        "        return obs\n",
        "\n",
        "    def render(self):\n",
        "        # In a real scenario, this would visualize the full state or observation\n",
        "        print(f\"Agent Pos: {self.agent_pos}, Goal Pos: {self.goal_pos}\")\n",
        "        print(f\"Current Observation Window:\\n{self._get_observation()}\")"
      ],
      "metadata": {
        "id": "p25yHdaPTric"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the custom environment\n",
        "env = PartiallyObservableGridEnv(maze_size=(10, 10), observation_window_size=3)\n",
        "\n",
        "# Run a sample episode with random actions\n",
        "observation, info = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "total_reward = 0\n",
        "\n",
        "while not terminated and not truncated:\n",
        "    env.render() # See the agent's limited view\n",
        "    action = env.action_space.sample() # Replace with your RL agent's policy\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    print(f\"Action taken: {action}, Reward received: {reward}\\n\")\n",
        "\n",
        "print(f\"Episode finished. Total Reward: {total_reward}\")\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8rMhWrITyQH",
        "outputId": "a6c191cd-9fa0-421b-a188-4eb4fc12b6fd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent Pos: (2, 4), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 3, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (2, 5), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 3, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (2, 6), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 1, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (3, 6), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 2, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (3, 5), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 1, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (4, 5), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 1, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (5, 5), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 3, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (5, 6), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 1, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (6, 6), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 1, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (7, 6), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 3, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (7, 7), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 3, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (7, 8), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 1, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (8, 8), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 1]]\n",
            "Action taken: 0, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (7, 8), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 1, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (8, 8), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 1]]\n",
            "Action taken: 0, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (7, 8), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 3, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (7, 9), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 2, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (7, 8), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Action taken: 1, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (8, 8), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 1]]\n",
            "Action taken: 3, Reward received: 0.0\n",
            "\n",
            "Agent Pos: (8, 9), Goal Pos: (9, 9)\n",
            "Current Observation Window:\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 1 0]]\n",
            "Action taken: 1, Reward received: 1.0\n",
            "\n",
            "Episode finished. Total Reward: 1.0\n"
          ]
        }
      ]
    }
  ]
}